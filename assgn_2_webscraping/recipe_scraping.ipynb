{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Recipe Website Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 15:58:10,231 - INFO - Starting the scraper...\n",
      "2025-02-26 15:58:10,232 - INFO - Fetching links from https://www.epicurious.com/recipes-menus...\n",
      "2025-02-26 15:58:11,055 - INFO - Found 38 links matching 'recipes-menus'\n",
      "2025-02-26 15:58:11,056 - INFO - [1/10] Scraping menu page: https://www.epicurious.com/recipes-menus\n",
      "2025-02-26 15:58:11,057 - INFO - Sleeping for 4.49 seconds to avoid detection...\n",
      "2025-02-26 15:58:15,545 - INFO - Fetching links from https://www.epicurious.com/recipes-menus...\n",
      "2025-02-26 15:58:16,029 - INFO - Found 19 links matching 'recipes/food/views'\n",
      "2025-02-26 15:58:16,030 - INFO - [2/10] Scraping menu page: https://www.epicurious.com/recipes-menus\n",
      "2025-02-26 15:58:16,031 - INFO - Sleeping for 4.13 seconds to avoid detection...\n",
      "2025-02-26 15:58:20,159 - INFO - Fetching links from https://www.epicurious.com/recipes-menus...\n",
      "2025-02-26 15:58:20,540 - INFO - Found 19 links matching 'recipes/food/views'\n",
      "2025-02-26 15:58:20,541 - INFO - [3/10] Scraping menu page: https://www.epicurious.com/recipes-menus/these-sheet-pan-turkey-wings-come-with-built-in-gravy\n",
      "2025-02-26 15:58:20,542 - INFO - Sleeping for 4.04 seconds to avoid detection...\n",
      "2025-02-26 15:58:24,580 - INFO - Fetching links from https://www.epicurious.com/recipes-menus/these-sheet-pan-turkey-wings-come-with-built-in-gravy...\n",
      "2025-02-26 15:58:25,261 - INFO - Found 5 links matching 'recipes/food/views'\n",
      "2025-02-26 15:58:25,263 - INFO - [4/10] Scraping menu page: https://www.epicurious.com/recipes-menus\n",
      "2025-02-26 15:58:25,264 - INFO - Sleeping for 4.27 seconds to avoid detection...\n",
      "2025-02-26 15:58:29,533 - INFO - Fetching links from https://www.epicurious.com/recipes-menus...\n",
      "2025-02-26 15:58:29,984 - INFO - Found 19 links matching 'recipes/food/views'\n",
      "2025-02-26 15:58:29,986 - INFO - [5/10] Scraping menu page: https://www.epicurious.com/story/recipes-menus/no-one-makes-mashed-potatoes-like-my-instant-pot\n",
      "2025-02-26 15:58:29,987 - INFO - Sleeping for 2.01 seconds to avoid detection...\n",
      "2025-02-26 15:58:31,995 - INFO - Fetching links from https://www.epicurious.com/story/recipes-menus/no-one-makes-mashed-potatoes-like-my-instant-pot...\n",
      "2025-02-26 15:58:32,602 - INFO - Found 3 links matching 'recipes/food/views'\n",
      "2025-02-26 15:58:32,603 - INFO - Total unique recipes found: 5\n",
      "2025-02-26 15:58:32,604 - INFO - [1/5] Scraping recipe page...\n",
      "2025-02-26 15:58:32,605 - INFO - Scraping recipe: https://www.epicurious.com/recipes/food/views/chocolate-chip-banana-bread-with-walnuts\n",
      "2025-02-26 15:58:32,606 - INFO - Sleeping for 2.96 seconds to avoid detection...\n",
      "2025-02-26 15:58:36,987 - INFO - [2/5] Scraping recipe page...\n",
      "2025-02-26 15:58:36,988 - INFO - Scraping recipe: https://www.epicurious.com/recipes/food/views/hot-buttered-rum\n",
      "2025-02-26 15:58:36,988 - INFO - Sleeping for 3.71 seconds to avoid detection...\n",
      "2025-02-26 15:58:41,314 - INFO - [3/5] Scraping recipe page...\n",
      "2025-02-26 15:58:41,315 - INFO - Scraping recipe: https://www.epicurious.com/recipes/food/views/spicy-honey-orange-shrimp\n",
      "2025-02-26 15:58:41,315 - INFO - Sleeping for 4.70 seconds to avoid detection...\n",
      "2025-02-26 15:58:46,619 - INFO - [4/5] Scraping recipe page...\n",
      "2025-02-26 15:58:46,620 - INFO - Scraping recipe: https://www.epicurious.com/recipes/food/views/passion-fruit-olive-oil-cake\n",
      "2025-02-26 15:58:46,621 - INFO - Sleeping for 4.99 seconds to avoid detection...\n",
      "2025-02-26 15:58:52,322 - INFO - [5/5] Scraping recipe page...\n",
      "2025-02-26 15:58:52,323 - INFO - Scraping recipe: https://www.epicurious.com/recipes/food/views/tuna-melt-nachos\n",
      "2025-02-26 15:58:52,324 - INFO - Sleeping for 4.65 seconds to avoid detection...\n",
      "2025-02-26 15:58:57,587 - INFO - Scraping completed! Data saved to 'epicurious_recipes.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.epicurious.com\"\n",
    "\n",
    "# Headers to avoid being blocked\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Delay function (random 2-5 sec pause)\n",
    "def delay():\n",
    "    sleep_time = random.uniform(2, 5)\n",
    "    logging.info(f\"Sleeping for {sleep_time:.2f} seconds to avoid detection...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# Step 1: Scrape all \"recipes-menus\" links from the main page\n",
    "def get_links(url, keyword):\n",
    "    try:\n",
    "        logging.info(f\"Fetching links from {url}...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [urljoin(base_url, a[\"href\"]) for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        logging.info(f\"Found {len(links)} links matching '{keyword}'\")\n",
    "        return links\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error fetching links from {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Step 2: Scrape \"recipes/food/views\" links from each \"recipes-menus\" page\n",
    "def get_recipe_links(menu_links):\n",
    "    recipe_links = []\n",
    "    for index, menu_link in enumerate(menu_links[:5]):  # Limit to first 10 links\n",
    "        logging.info(f\"[{index+1}/10] Scraping menu page: {menu_link}\")\n",
    "        delay()\n",
    "        recipe_links.extend(get_links(menu_link, \"recipes/food/views\"))\n",
    "    \n",
    "    recipe_links = list(set(recipe_links))[:5]  # Limit to first 10 unique recipe links\n",
    "    logging.info(f\"Total unique recipes found: {len(recipe_links)}\")\n",
    "    return recipe_links\n",
    "\n",
    "# Step 3: Scrape ingredients and preparation steps from each recipe page\n",
    "def scrape_recipe(url):\n",
    "    try:\n",
    "        logging.info(f\"Scraping recipe: {url}\")\n",
    "        delay()\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Get ingredients\n",
    "        ingredients_container = soup.find(\"div\", {\"data-testid\": \"IngredientList\"})\n",
    "        ingredients = [item.get_text(strip=True) for item in ingredients_container.find_all(\"div\", class_=\"Description-cSrMCf\")] if ingredients_container else [\"Not found\"]\n",
    "\n",
    "        # Get preparation steps\n",
    "        instructions_container = soup.find(\"div\", {\"data-testid\": \"InstructionsWrapper\"})\n",
    "        step_titles = instructions_container.find_all(\"h4\") if instructions_container else []\n",
    "        step_texts = instructions_container.find_all(\"p\") if instructions_container else []\n",
    "        steps = [f\"{title.get_text(strip=True)}: {text.get_text(strip=True)}\" for title, text in zip(step_titles, step_texts)] if step_titles else [\"Not found\"]\n",
    "\n",
    "        return {\"url\": url, \"ingredients\": ingredients, \"steps\": steps}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error scraping recipe {url}: {e}\")\n",
    "        return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "# Run the scraper\n",
    "logging.info(\"Starting the scraper...\")\n",
    "menu_links = get_links(\"https://www.epicurious.com/recipes-menus\", \"recipes-menus\")\n",
    "recipe_links = get_recipe_links(menu_links)\n",
    "\n",
    "recipes_data = []\n",
    "for index, recipe_link in enumerate(recipe_links):\n",
    "    logging.info(f\"[{index+1}/{len(recipe_links)}] Scraping recipe page...\")\n",
    "    recipes_data.append(scrape_recipe(recipe_link))\n",
    "\n",
    "# Save to JSON file\n",
    "json_file = \"epicurious_recipes.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(recipes_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "logging.info(f\"Scraping completed! Data saved to '{json_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
